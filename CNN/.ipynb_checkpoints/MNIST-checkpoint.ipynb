{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cf4e71-46ae-4319-bc4b-2aaf90d5ebe4",
   "metadata": {},
   "source": [
    "Notes to self \n",
    "need to pip install torch and torchvision along with numpy and matplotlib\n",
    "For visualizing \n",
    "https://adamharley.com/nn_vis/cnn/3d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ed3d37-5f55-40ae-93ad-c1937e8d8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b92b23a-33f1-4247-9965-acea8108b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 9.91M/9.91M [00:15<00:00, 647kB/s]\n",
      "100%|██████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 97.3kB/s]\n",
      "100%|███████████████████████████████████████| 1.65M/1.65M [00:06<00:00, 274kB/s]\n",
      "100%|██████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 5.91MB/s]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266325fd-1186-44d3-b5bf-ff8ad7a70cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e57f0b1-6bb7-4fc1-8e8e-d15f06bcc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class\n",
    "class CNN(nn.Module):\n",
    "     # Initialization\n",
    "    def __init__(self):\n",
    "        super (CNN, self).__init__()\n",
    "        \n",
    "        self.conv1_out_np = np.zeros((1, 3, 24, 24))\n",
    "        self.mp1_out_np = np.zeros((1, 3, 12, 12))\n",
    "        self.conv2_out_np = np.zeros((1, 3, 8, 8))\n",
    "        self.mp2_out_np = np.zeros((1, 3, 4, 4))\n",
    "        self.fc_in_np = np.zeros((1, 48))\n",
    "        self.fc_out_np = np.zeros((1, 10))\n",
    "        \n",
    "        # 1st Convolution Layer\n",
    "        # Image Input Shape -> (28, 28, 1)\n",
    "        # Convolution Layer -> (24, 24, 3)\n",
    "        # Pooling Max Layer -> (12, 12, 3)\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=5)\n",
    "        \n",
    "        # 2nd Convolution Layer\n",
    "        # Image Input Shape -> (12, 12, 3)\n",
    "        # Convolution Layer -> (8, 8, 3)\n",
    "        # pooling Max Layer -> (4, 4, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 3, kernel_size=5)\n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        # Num of Weight = 480\n",
    "        self.fc_1 = nn.Linear(48, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        # Layer Integration\n",
    "        x = self.conv1(x)\n",
    "        self.conv1_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp1_out_np = x.detach().numpy()\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        self.conv2_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp2_out_np = x.detach().numpy()\n",
    "        \n",
    "        # Flatten Layer\n",
    "        x = x.view(in_size, -1)\n",
    "        self.fc_in_np = x.detach().numpy()\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        x = self.fc_1(x)\n",
    "        self.fc_out_np = x.detach().numpy()\n",
    "        \n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563d1463-34f2-4d02-8c7e-cb1d231b9e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiation\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407fde9-b964-473f-bc2e-7cab474cdc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 796\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d3add8-240b-4757-90e9-c098701a77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3eeb0e-3f69-471d-8956-5039be432bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ouput of feedforwarding\n",
    "        output = model(data)\n",
    "        \n",
    "        # Loss calibration\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Back propagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a6823f-d880-4766-8436-46204b2f97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # Output of feedforwarding\n",
    "        output = model(data)\n",
    "        \n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "          \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e91c2e-03c1-45cb-a776-5fe0d163ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3371193811.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305046\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.294673\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.310417\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.281048\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.313326\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.277228\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.266124\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.266945\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.257721\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.263060\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.199957\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.256432\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.155236\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.119855\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.034079\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.986620\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.839647\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.696462\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.635875\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.235895\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.421611\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.227804\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.751514\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.916342\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.870232\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.838340\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.744977\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.652304\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.650338\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.727601\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.875248\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.635524\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.404689\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.611676\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.490975\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.581124\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.704374\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.434379\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.402231\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.561933\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.689300\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.481485\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.762755\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.783186\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.584455\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.605876\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.536317\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.312642\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.521396\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.531019\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.519086\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.476368\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.613416\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.694631\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.208698\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.432352\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.347795\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.491867\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.460231\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.368110\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.356393\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.508165\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.387619\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.600336\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.498613\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.561792\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.615159\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.360305\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.622237\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.468100\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.476723\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.539664\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.345542\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.521934\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.441961\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.396805\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.295766\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.432231\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.380275\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.694446\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.545125\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.219925\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.532201\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.819347\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.314470\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.216957\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.245687\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.240483\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.516939\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.178253\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.307176\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.300316\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.272371\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.231537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3681559039.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3557, Accuracy: 8866/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.477149\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.282712\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.429917\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.163213\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.367151\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.291339\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.327956\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.230496\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.364365\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.320786\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.256893\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.517093\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.387233\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.639508\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.285001\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.556778\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.457027\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.270714\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.166826\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.327572\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.113169\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.307383\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.396565\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.144203\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.341052\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.348456\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.241059\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.248100\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.413379\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.195710\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.201542\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.208653\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.360783\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.188033\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.287648\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.289113\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.293773\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.143593\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.246097\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.247070\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.248761\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.419064\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.256662\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.484837\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.447373\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.522170\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.307728\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.398902\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.342540\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.328038\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.638311\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.275950\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.456083\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.283969\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.206175\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.234412\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.178658\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.276272\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.520194\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.275647\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.372294\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.403565\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.447600\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.175363\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.251685\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.311677\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.334684\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.137266\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.188194\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.233612\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.217034\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.337403\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.312108\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.244375\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.326133\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.164146\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.290541\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.259803\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.097230\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.194192\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.253981\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.106915\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.255427\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.223344\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.478462\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.377089\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.152805\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.285903\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.486217\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.294185\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.110720\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.272110\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.348141\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.251361\n",
      "\n",
      "Test set: Average loss: 0.2249, Accuracy: 9302/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.179008\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.259856\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.236694\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.272829\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.385323\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.339846\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.275236\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.278183\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.166531\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.185217\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.136273\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.276792\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.126751\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.248353\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.288705\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.140592\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.253860\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.224079\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.378813\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.109433\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.317211\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.167052\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.252149\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.247703\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.267276\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.134653\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.192087\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.101621\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.114332\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.264559\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.259409\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.095437\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.259572\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.196908\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.054478\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.227356\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.254478\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.195303\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.257450\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.208449\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.227733\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.462812\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.102038\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.230523\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.139429\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.146107\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.219064\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.177739\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.269800\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.311078\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.119321\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.250363\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.143956\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.158426\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.132787\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.184885\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.226917\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.279376\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.170263\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.196618\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.335744\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.114624\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.252728\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.317131\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.127512\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.130088\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.303076\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.210434\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.120357\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.208783\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.214730\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.300144\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.150019\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.212711\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.137299\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.241089\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.540435\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.309063\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.228328\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.272357\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.168033\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.150319\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.100783\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.190143\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.157539\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.358432\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.182222\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.329374\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.254658\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.112072\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.166732\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.317005\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.349877\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.080531\n",
      "\n",
      "Test set: Average loss: 0.1899, Accuracy: 9403/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.214623\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.281779\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.271008\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.202778\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.161300\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.241020\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.378070\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.156393\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.332801\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.159408\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.227286\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.129910\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.125975\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.132745\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.306770\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.361140\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.174568\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.203360\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.134260\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.169927\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.142273\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.256152\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.193737\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.247669\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.139289\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.181130\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.360638\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.307432\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.098261\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.295794\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.287787\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.138918\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.281730\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.329445\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.199875\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.185413\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.253205\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.307956\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.055452\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.197725\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.124876\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.172236\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.119299\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.095031\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.127337\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.252949\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.103924\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.097845\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.355908\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.139161\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.078903\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.201066\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.183021\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.105908\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.357899\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.345530\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.092968\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.375963\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.153064\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.109327\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.105062\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.097820\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.115868\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.212100\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.084199\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.165220\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.174397\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.156061\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.145458\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.200274\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.506151\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.356243\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.101067\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.135904\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.182674\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.212901\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.185658\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.216975\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.172618\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.191878\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.107082\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.155761\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.423141\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.260315\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.214421\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.054368\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.113783\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.261692\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.176224\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.131632\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.231992\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.098671\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.266921\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.193135\n",
      "\n",
      "Test set: Average loss: 0.1813, Accuracy: 9416/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.180221\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.119565\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.078969\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.245044\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.302953\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.080546\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.284823\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.216073\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.150736\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.070512\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.097358\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.315506\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.199803\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.229917\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.267794\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.135681\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.165094\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.200916\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.111916\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.123413\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.165927\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.334109\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.157410\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.129267\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.169505\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.194359\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.077786\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.211727\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.069262\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.208548\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.225202\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.196427\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.085226\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.180763\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.123965\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.128378\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.279062\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.092460\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.214340\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.046309\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.150046\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.327949\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.113911\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.261249\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.073684\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.186719\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.133052\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.229216\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.270015\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.176728\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.161319\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.085305\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.272409\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.156214\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.065465\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.111580\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.204862\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.139785\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.208776\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.288874\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.243888\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.079135\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.242752\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.209368\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.253370\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.134043\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.197592\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.070391\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.320649\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.107532\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.099576\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.140105\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.102624\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.303540\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.187990\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.113650\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.306923\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.329693\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.336867\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.175727\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.097240\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.228960\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.278632\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.195115\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.224730\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.064797\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.157089\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.223903\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.238081\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.366019\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.102310\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.097693\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.216340\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.108958\n",
      "\n",
      "Test set: Average loss: 0.1624, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.118761\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.087952\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.212353\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.127475\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.111845\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.162747\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.210339\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.233694\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.062490\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.353766\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.208497\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.202189\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.171855\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.109148\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.240524\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.074085\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.108003\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.144405\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.273586\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.264778\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.314563\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.144632\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.153590\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.159133\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.175728\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.225605\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.148658\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.198181\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.135634\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.187674\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.219120\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.169697\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.110838\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.082903\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.372591\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.096408\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.050934\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.146941\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.187503\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.061087\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.291219\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.140412\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.205291\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.170498\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.114564\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.220162\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.149971\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.088681\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.075209\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.135309\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.176742\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.269495\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.186273\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.271427\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.143411\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.083000\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.108708\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.142497\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.115447\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.038942\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.268993\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.152767\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.211292\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.059689\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.144103\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.344550\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.086527\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.203508\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.070805\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.162803\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.197640\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.274932\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.285965\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.180282\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.052845\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.284843\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.085027\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.067779\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.129566\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.083214\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.307080\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.080262\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.095650\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.210243\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.205637\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.224539\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.051378\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.089936\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.132597\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.153913\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.095824\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.176860\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.087537\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.197115\n",
      "\n",
      "Test set: Average loss: 0.1493, Accuracy: 9551/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.148249\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.425320\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.153671\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.070272\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.161580\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.249372\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.158635\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.362463\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.037520\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.183628\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.178781\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.249738\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.105443\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.210505\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.071230\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.301027\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.045725\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.115807\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.056605\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.125762\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.098067\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.130838\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.068793\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.084500\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.200359\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.233644\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.392255\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.133014\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.212923\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.120785\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.246184\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.128583\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.091509\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.259831\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.208096\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.104429\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.195244\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.178245\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.090028\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.103034\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.308786\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.116158\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.153244\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.191216\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.087754\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.151228\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.088773\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.049449\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.160527\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.099203\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.216515\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.167597\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.124993\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.117312\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.093647\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.092574\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.234480\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.209915\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.229057\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.159072\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.166154\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.173333\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.236880\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.102620\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.106509\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.049835\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.079318\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.063878\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.147633\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.184111\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.151281\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.195415\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.209375\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.143032\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.050612\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.106185\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.112541\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.103424\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.068424\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.123551\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.144217\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.157761\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.213485\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.323180\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.195275\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.145396\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.190561\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.197205\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.058394\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.122054\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.122122\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.171342\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.298592\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.160430\n",
      "\n",
      "Test set: Average loss: 0.1481, Accuracy: 9525/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.079240\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.076970\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.061442\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.100338\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.219575\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.242292\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.106266\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.130600\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.101849\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.085775\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.190473\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.148117\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.055634\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.343293\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.376449\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.099733\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.296511\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.162234\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.206593\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.214030\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.107176\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.245631\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.119917\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.235706\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.084979\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.226594\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.298692\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.077301\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.220347\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.077260\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.291643\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.239626\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.094754\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.173435\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.411143\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.135579\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.272379\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.111368\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.146714\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.145025\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.190069\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.180063\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.163759\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.204654\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.159625\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.066115\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.190953\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.157805\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.208401\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.142225\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.096404\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.245972\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.286430\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.173598\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.067429\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.165992\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.099905\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.173305\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.151739\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.096365\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.103411\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.095079\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.172610\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.234140\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.131132\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.051782\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.181382\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.152906\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.073707\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.163409\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.099732\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.071092\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.260255\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.184165\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.312206\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.168914\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.141539\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.158404\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.082813\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.045306\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.089908\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.229451\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.072564\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.154259\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.056029\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.052450\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.277483\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.102058\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.184863\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.105196\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.192969\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.271207\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.083691\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.244849\n",
      "\n",
      "Test set: Average loss: 0.1454, Accuracy: 9557/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.108377\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.206054\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.176935\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.135906\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.091287\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.080523\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.193620\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.069723\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.192652\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.165443\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.188259\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.119648\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.266350\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.139652\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.103432\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.161253\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.038385\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.200428\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.239229\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.276142\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.182816\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.069734\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.125943\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.244422\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.243486\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.231637\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.235574\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.086867\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.296544\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.148618\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.107603\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.175466\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.066677\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.222485\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.177344\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.093451\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.168711\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.089253\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.051764\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.078564\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.123728\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.045760\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.109903\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.282993\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.059656\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.136260\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.133802\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.066918\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.091022\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.095611\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.041221\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.159355\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.135860\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.110336\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.175459\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.110080\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.059103\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.175323\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.127344\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.251368\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.136199\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.096365\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.175415\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.206231\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.110643\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.328446\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.070778\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.175999\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.118509\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.141825\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.183873\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.193318\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.095080\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.332053\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.175367\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.262282\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.123344\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.149900\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.120134\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.313736\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.163397\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.043204\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.154083\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.129815\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.144376\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.060985\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.264965\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.207539\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.140232\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.192390\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.204599\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.185715\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.087051\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.148210\n",
      "\n",
      "Test set: Average loss: 0.1498, Accuracy: 9535/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traning process\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05eb9a3-62ba-48e1-be87-5feaaba9651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, \"./cnn_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa75e61-29de-4784-a398-4bcddd255c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = torch.load(\"./cnn_mnist.pt\", weights_only=False)\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d8dd1be-e4f1-4244-a8d6-12ab5f7ee5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "218fc6a3-9bca-4e9a-ae4e-11baf2d29f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x324724ec0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load bitmap image\n",
    "img = Image.open(\"./model/bmp/train_0.bmp\", \"r\")\n",
    "np_img = np.array(img)\n",
    "pyplot.imshow(np_img, cmap=pyplot.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65911ad3-c72e-47f9-9e96-a144d8057554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3371193811.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "np_img_re = np.reshape(np_img, (1,1,28,28))\n",
    "# 0 - 255 => 0 - 1 로 정규화, np.array => tensor 변환\n",
    "data = Variable(torch.tensor((np_img_re / 255), dtype = torch.float32))\n",
    "    \n",
    "# Output of feedforwarding\n",
    "output = model(data)\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "print('Predicted output: ' + ', '.join(map(str, pred.flatten().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d61c1d-e2ba-4218-b106-c6a09b820d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[ 19,  52,  49,  92,  89],\n",
      "        [ 11,  19,   8,  47,  45],\n",
      "        [-43, -10,  12,  51,  31],\n",
      "        [-11,   4,  16,  60,  53],\n",
      "        [-32,  16,  16,   2,  13]], dtype=torch.int32)\n",
      "tensor([[-16,  -8,  -9,  15,  19],\n",
      "        [  6,  16,  24,  35,  21],\n",
      "        [  8,  56, 116,  50,   9],\n",
      "        [ 64,  90, 122,  62,  15],\n",
      "        [ 73,  85,  40,  23,   7]], dtype=torch.int32)\n",
      "tensor([[ 41,  31,  -7,  26,  -9],\n",
      "        [  2,  10, -13,  12,  -2],\n",
      "        [ 14,  12, -34, -23, -39],\n",
      "        [  6,  -4, -28,  -1, -37],\n",
      "        [ 26, -15,  -2, -25, -14]], dtype=torch.int32)\n",
      "tensor([ 0,  0, 47], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[ 19,  52,  49,  92,  89],\n",
      "        [ 11,  19,   8,  47,  45],\n",
      "        [213, 246,  12,  51,  31],\n",
      "        [245,   4,  16,  60,  53],\n",
      "        [224,  16,  16,   2,  13]], dtype=torch.int32)\n",
      "tensor([[240, 248, 247,  15,  19],\n",
      "        [  6,  16,  24,  35,  21],\n",
      "        [  8,  56, 116,  50,   9],\n",
      "        [ 64,  90, 122,  62,  15],\n",
      "        [ 73,  85,  40,  23,   7]], dtype=torch.int32)\n",
      "tensor([[ 41,  31, 249,  26, 247],\n",
      "        [  2,  10, 243,  12, 254],\n",
      "        [ 14,  12, 222, 233, 217],\n",
      "        [  6, 252, 228, 255, 219],\n",
      "        [ 26, 241, 254, 231, 242]], dtype=torch.int32)\n",
      "tensor([ 0,  0, 47], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/359728266.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_1 = torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/359728266.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_2 = torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/359728266.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_3 = torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/359728266.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_bias = torch.tensor((model.conv1.bias.data * 128), dtype = torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#################### Weight & Bias in HEX of Convolution Layer1 ####################\n",
    "\n",
    "# Calibration\n",
    "int_conv1_weight_1 = torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_2 = torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_3 = torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv1_bias = torch.tensor((model.conv1.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print(\"Signed\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "# 2's Complement\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv1_weight_1[i][j] < 0:\n",
    "            int_conv1_weight_1[i][j] += 256\n",
    "        if int_conv1_weight_2[i][j] < 0:\n",
    "            int_conv1_weight_2[i][j] += 256\n",
    "        if int_conv1_weight_3[i][j] < 0:\n",
    "            int_conv1_weight_3[i][j] += 256\n",
    "\n",
    "for k in range(3):\n",
    "    if int_conv1_bias[k] < 0:\n",
    "        int_conv1_bias[k] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "np.savetxt('conv1_weight_1.mem', int_conv1_weight_1, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_2.mem', int_conv1_weight_2, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_3.mem', int_conv1_weight_3, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_bias.mem', int_conv1_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9af2d4-4642-4733-8466-f7007a03be75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[ 21,  34,  44,  25,  30],\n",
      "        [  0,  -2, -18, -33,   9],\n",
      "        [-11, -57, -20,   2,  25],\n",
      "        [-27, -20,  19,   7,   0],\n",
      "        [-27,  -4,  23,  19,  -4]], dtype=torch.int32)\n",
      "tensor([[-29,  -4,  -2, -49, -71],\n",
      "        [ 20,  -1, -79, -67,  54],\n",
      "        [ 37, -49, -54,  26,  72],\n",
      "        [ 12, -62,  -9,  36,  -9],\n",
      "        [ 24,  -4,  28,  33,  14]], dtype=torch.int32)\n",
      "tensor([[ 13,  17,   0,  30,   1],\n",
      "        [  6,  20,  12,   2,  -8],\n",
      "        [ 15,  12,  -4,  -2,  22],\n",
      "        [ 16,  24,   9,  -3,   3],\n",
      "        [ 26,  16,   7,   5, -17]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  7,  15,  16,   9,  11],\n",
      "        [-18, -10,  -4,   8,  15],\n",
      "        [-51, -73, -52, -22, -25],\n",
      "        [ 39,  14,  18,  32,  45],\n",
      "        [  9,   7,  22,   6, -28]], dtype=torch.int32)\n",
      "tensor([[-22, -17,   7,   0, -30],\n",
      "        [-12, -36, -68, -69, -33],\n",
      "        [ 34,   7,  33,  50,  21],\n",
      "        [ 30,  75,  76,  43, -16],\n",
      "        [-15, -24, -15, -26,  -3]], dtype=torch.int32)\n",
      "tensor([[-14, -10,   2,  -9,   9],\n",
      "        [ 25,  17,  16,   3,  13],\n",
      "        [ -4, -16, -29,  -7,   3],\n",
      "        [ -7, -10,  -2,   9,   0],\n",
      "        [ -7,   7,   7, -17, -13]], dtype=torch.int32) \n",
      "\n",
      "tensor([[-12, -18, -43, -18, -42],\n",
      "        [  7,  27,  -3,   7, -15],\n",
      "        [ -6,  -3,  23,  54,   3],\n",
      "        [-17, -29,  26,  28,  -2],\n",
      "        [-17, -27,  13,  27,  -7]], dtype=torch.int32)\n",
      "tensor([[ 14,  19,  -1, -11, -15],\n",
      "        [-13,  11,   4,  15, -17],\n",
      "        [ -1,  17,  -8,  41, -22],\n",
      "        [  6,   6, -22,  26,  38],\n",
      "        [ -1,  18,  38,  50,  40]], dtype=torch.int32)\n",
      "tensor([[ -3, -13,  -9,  -1,   6],\n",
      "        [ 10, -15, -15,   0,  15],\n",
      "        [-15, -26, -25, -16,  10],\n",
      "        [  7, -12,  -7, -11,   3],\n",
      "        [ -5, -19,   6,  -8,   6]], dtype=torch.int32) \n",
      "\n",
      "tensor([ 10,   4, -16], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[ 21,  34,  44,  25,  30],\n",
      "        [  0, 254, 238, 223,   9],\n",
      "        [245, 199, 236,   2,  25],\n",
      "        [229, 236,  19,   7,   0],\n",
      "        [229, 252,  23,  19, 252]], dtype=torch.int32)\n",
      "tensor([[227, 252, 254, 207, 185],\n",
      "        [ 20, 255, 177, 189,  54],\n",
      "        [ 37, 207, 202,  26,  72],\n",
      "        [ 12, 194, 247,  36, 247],\n",
      "        [ 24, 252,  28,  33,  14]], dtype=torch.int32)\n",
      "tensor([[ 13,  17,   0,  30,   1],\n",
      "        [  6,  20,  12,   2, 248],\n",
      "        [ 15,  12, 252, 254,  22],\n",
      "        [ 16,  24,   9, 253,   3],\n",
      "        [ 26,  16,   7,   5, 239]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  7,  15,  16,   9,  11],\n",
      "        [238, 246, 252,   8,  15],\n",
      "        [205, 183, 204, 234, 231],\n",
      "        [ 39,  14,  18,  32,  45],\n",
      "        [  9,   7,  22,   6, 228]], dtype=torch.int32)\n",
      "tensor([[234, 239,   7,   0, 226],\n",
      "        [244, 220, 188, 187, 223],\n",
      "        [ 34,   7,  33,  50,  21],\n",
      "        [ 30,  75,  76,  43, 240],\n",
      "        [241, 232, 241, 230, 253]], dtype=torch.int32)\n",
      "tensor([[242, 246,   2, 247,   9],\n",
      "        [ 25,  17,  16,   3,  13],\n",
      "        [252, 240, 227, 249,   3],\n",
      "        [249, 246, 254,   9,   0],\n",
      "        [249,   7,   7, 239, 243]], dtype=torch.int32) \n",
      "\n",
      "tensor([[244, 238, 213, 238, 214],\n",
      "        [  7,  27, 253,   7, 241],\n",
      "        [250, 253,  23,  54,   3],\n",
      "        [239, 227,  26,  28, 254],\n",
      "        [239, 229,  13,  27, 249]], dtype=torch.int32)\n",
      "tensor([[ 14,  19, 255, 245, 241],\n",
      "        [243,  11,   4,  15, 239],\n",
      "        [255,  17, 248,  41, 234],\n",
      "        [  6,   6, 234,  26,  38],\n",
      "        [255,  18,  38,  50,  40]], dtype=torch.int32)\n",
      "tensor([[253, 243, 247, 255,   6],\n",
      "        [ 10, 241, 241,   0,  15],\n",
      "        [241, 230, 231, 240,  10],\n",
      "        [  7, 244, 249, 245,   3],\n",
      "        [251, 237,   6, 248,   6]], dtype=torch.int32) \n",
      "\n",
      "tensor([ 10,   4, 240], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_11 = torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_12 = torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_13 = torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_21 = torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_22 = torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_23 = torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_31 = torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_32 = torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_33 = torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
      "/var/folders/n2/t1t41mvs7_v9x0pq82l_hsgh0000gn/T/ipykernel_3424/3533637025.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_bias = torch.tensor((model.conv2.bias.data * 128), dtype = torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#################### Weight & Bias in HEX of Convolution Layer2 ####################\n",
    "\n",
    "# Calibration\n",
    "# print(np.shape(model.conv2.weight))\n",
    "int_conv2_weight_11 = torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_12 = torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_13 = torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_21 = torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_22 = torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_23 = torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_31 = torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_32 = torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_33 = torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_bias = torch.tensor((model.conv2.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print (\"Signed\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "# 2's Complement\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv2_weight_11[i][j] < 0:\n",
    "            int_conv2_weight_11[i][j] += 256\n",
    "        if int_conv2_weight_12[i][j] < 0:\n",
    "            int_conv2_weight_12[i][j] += 256\n",
    "        if int_conv2_weight_13[i][j] < 0:\n",
    "            int_conv2_weight_13[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_21[i][j] < 0:\n",
    "            int_conv2_weight_21[i][j] += 256\n",
    "        if int_conv2_weight_22[i][j] < 0:\n",
    "            int_conv2_weight_22[i][j] += 256\n",
    "        if int_conv2_weight_23[i][j] < 0:\n",
    "            int_conv2_weight_23[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_31[i][j] < 0:\n",
    "            int_conv2_weight_31[i][j] += 256\n",
    "        if int_conv2_weight_32[i][j] < 0:\n",
    "            int_conv2_weight_32[i][j] += 256\n",
    "        if int_conv2_weight_33[i][j] < 0:\n",
    "            int_conv2_weight_33[i][j] += 256\n",
    "\n",
    "for k in range(3):\n",
    "    if int_conv2_bias[k] < 0:\n",
    "        int_conv2_bias[k] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "np.savetxt('conv2_weight_11.mem', int_conv2_weight_11, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_12.mem', int_conv2_weight_12, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_13.mem', int_conv2_weight_13, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_21.mem', int_conv2_weight_21, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_22.mem', int_conv2_weight_22, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_23.mem', int_conv2_weight_23, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_31.mem', int_conv2_weight_31, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_32.mem', int_conv2_weight_32, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_33.mem', int_conv2_weight_33, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_bias.mem', int_conv2_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d76a259f-b233-427a-b916-80ff4aee5b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n",
      "tensor([[ 17,  10,   1, -20,   5,   4,   0,  21,  15,   2,  11,  25, -45, -31,\n",
      "          -9,  21,  10,   0,  -6,   2,  19, -17, -57, -26, -35, -18,   0,   2,\n",
      "         -18,  15,  24,  17,  24, -19,   2,  26,   8, -51,   0,  50,  31,  -9,\n",
      "          24, -23,  33, -18, -24, -23],\n",
      "        [-11,  20,  58,  31,  -5,   4,   9,  -9,  16,  12,  -9,  -6,   0,   7,\n",
      "         -14,  -2, -33,  14, -51, -17,   1, -19, -37, -26,  -7,  13, -38,  18,\n",
      "          45, -11,   1,  27, -16,  65, -25, -45, -15,  27,  -7,  -4,  -4,  42,\n",
      "         -34,  14, -20, -16,   8,  -3],\n",
      "        [-28, -34, -45, -13,  14, -12,  20,  16,  41,  22,  13,  20,   1,   9,\n",
      "         -44, -31,  -4,  18,  -5, -25,  51,   2,   2, -16,  48, -12,  -2,  33,\n",
      "         -22,   7,   9,  57, -14, -21,  22,   1, -25,  13,  15,  16,  36,  25,\n",
      "         -15, -11, -17, -54,   0,  18],\n",
      "        [-43, -33, -35,  21,  33,  18,   0, -31,  10,  -9, -45, -19,  -8,  -3,\n",
      "           5, -10,   2,  46,   2,  -6,  15,  11,  20, -24,  17,  12,  13,   4,\n",
      "          36,  42,  25, -17, -13,  47,  35,   4, -39,  -7, -37,   5, -41, -18,\n",
      "          40,   2, -22,  16,  -3,   2],\n",
      "        [  3,  25,  70,  46,   5, -20,  -3, -11, -27, -10,  -5, -18,  17,   3,\n",
      "          -6,  -7,  11, -50,  -7, -60,  16,  -4,  -4,  11,  -8,  15,  11,  18,\n",
      "          -2, -19, -44, -17,  46, -22,  31, -21,  35,   2,  63,  -3, -33, -22,\n",
      "         -41, -15,  -3,   7,   7,  14],\n",
      "        [ 23,  27,  -5,   6, -27,  23, -51, -24, -18, -16, -16,   0, -12,  11,\n",
      "          30,  14, -29,   4,   5,  53, -16,  13,   1,  38,  21,  20,  12, -27,\n",
      "          36,  39,  29, -29,  39, -21, -77,  -8,  28, -35, -42, -41,  19,  23,\n",
      "          31,  24,   0,  13,   4,  20],\n",
      "        [ 37,  50,  43,  16,  12,  21,   0, -25,  -1,   4,  -4, -12, -54, -30,\n",
      "         -11,  -6,  -8, -33,  -1,  17, -24,  -6,   8,  40, -49, -25,  22,  -9,\n",
      "         -51,  -7,  33,   5,   8, -33, -78, -31,  -1,   0, -51,  33,  71,  22,\n",
      "          56,  31,  18, -13, -25, -44],\n",
      "        [-48, -36, -18,   9,  21,  21,  15,   1,  22,  38,  18, -17,  19,  16,\n",
      "          34,   8,  12,  21,  38,  15,  22,  24, -25,  20,  20, -14,  -7,  12,\n",
      "          26, -35, -58, -31,   3,  -6,  10, -10,   0, -25,  49,   0, -36, -13,\n",
      "         -34, -10,   0,  27,   1, -20],\n",
      "        [ 10,  -1, -31,   4, -14, -16,  -5,   5,  21,   0,   1, -34,  12, -14,\n",
      "         -19,   4,  15, -25,   7,  11, -18,   8,  31,  -2,  23, -26,  -1, -10,\n",
      "         -35,   2,  36, -17,   0, -14,  27,  39,   2,  56, -56, -20,  14, -13,\n",
      "          -3,  -4,   0,  14,  20,  29],\n",
      "        [ 48,   5, -42, -43,   1,   3,  22,  11, -67, -36,  29,  17,  17,   8,\n",
      "           5,  19,  30,   8,  -8,  17, -40,  -7,  38, -11, -57,  61, -21, -24,\n",
      "           7,  27, -31, -33, -46,   5,  47,   7, -22,  23,  20,   1, -31, -31,\n",
      "           0, -16,  -3,  -5,   4,  20]], dtype=torch.int32)\n",
      "torch.Size([10])\n",
      "tensor([ -6,   4, -11,  -6, -10,  19, -21, -12,  -2,  10], dtype=torch.int32)\n",
      "tensor([[ 17,  10,   1, 236,   5,   4,   0,  21,  15,   2,  11,  25, 211, 225,\n",
      "         247,  21,  10,   0, 250,   2,  19, 239, 199, 230, 221, 238,   0,   2,\n",
      "         238,  15,  24,  17,  24, 237,   2,  26,   8, 205,   0,  50,  31, 247,\n",
      "          24, 233,  33, 238, 232, 233],\n",
      "        [245,  20,  58,  31, 251,   4,   9, 247,  16,  12, 247, 250,   0,   7,\n",
      "         242, 254, 223,  14, 205, 239,   1, 237, 219, 230, 249,  13, 218,  18,\n",
      "          45, 245,   1,  27, 240,  65, 231, 211, 241,  27, 249, 252, 252,  42,\n",
      "         222,  14, 236, 240,   8, 253],\n",
      "        [228, 222, 211, 243,  14, 244,  20,  16,  41,  22,  13,  20,   1,   9,\n",
      "         212, 225, 252,  18, 251, 231,  51,   2,   2, 240,  48, 244, 254,  33,\n",
      "         234,   7,   9,  57, 242, 235,  22,   1, 231,  13,  15,  16,  36,  25,\n",
      "         241, 245, 239, 202,   0,  18],\n",
      "        [213, 223, 221,  21,  33,  18,   0, 225,  10, 247, 211, 237, 248, 253,\n",
      "           5, 246,   2,  46,   2, 250,  15,  11,  20, 232,  17,  12,  13,   4,\n",
      "          36,  42,  25, 239, 243,  47,  35,   4, 217, 249, 219,   5, 215, 238,\n",
      "          40,   2, 234,  16, 253,   2],\n",
      "        [  3,  25,  70,  46,   5, 236, 253, 245, 229, 246, 251, 238,  17,   3,\n",
      "         250, 249,  11, 206, 249, 196,  16, 252, 252,  11, 248,  15,  11,  18,\n",
      "         254, 237, 212, 239,  46, 234,  31, 235,  35,   2,  63, 253, 223, 234,\n",
      "         215, 241, 253,   7,   7,  14],\n",
      "        [ 23,  27, 251,   6, 229,  23, 205, 232, 238, 240, 240,   0, 244,  11,\n",
      "          30,  14, 227,   4,   5,  53, 240,  13,   1,  38,  21,  20,  12, 229,\n",
      "          36,  39,  29, 227,  39, 235, 179, 248,  28, 221, 214, 215,  19,  23,\n",
      "          31,  24,   0,  13,   4,  20],\n",
      "        [ 37,  50,  43,  16,  12,  21,   0, 231, 255,   4, 252, 244, 202, 226,\n",
      "         245, 250, 248, 223, 255,  17, 232, 250,   8,  40, 207, 231,  22, 247,\n",
      "         205, 249,  33,   5,   8, 223, 178, 225, 255,   0, 205,  33,  71,  22,\n",
      "          56,  31,  18, 243, 231, 212],\n",
      "        [208, 220, 238,   9,  21,  21,  15,   1,  22,  38,  18, 239,  19,  16,\n",
      "          34,   8,  12,  21,  38,  15,  22,  24, 231,  20,  20, 242, 249,  12,\n",
      "          26, 221, 198, 225,   3, 250,  10, 246,   0, 231,  49,   0, 220, 243,\n",
      "         222, 246,   0,  27,   1, 236],\n",
      "        [ 10, 255, 225,   4, 242, 240, 251,   5,  21,   0,   1, 222,  12, 242,\n",
      "         237,   4,  15, 231,   7,  11, 238,   8,  31, 254,  23, 230, 255, 246,\n",
      "         221,   2,  36, 239,   0, 242,  27,  39,   2,  56, 200, 236,  14, 243,\n",
      "         253, 252,   0,  14,  20,  29],\n",
      "        [ 48,   5, 214, 213,   1,   3,  22,  11, 189, 220,  29,  17,  17,   8,\n",
      "           5,  19,  30,   8, 248,  17, 216, 249,  38, 245, 199,  61, 235, 232,\n",
      "           7,  27, 225, 223, 210,   5,  47,   7, 234,  23,  20,   1, 225, 225,\n",
      "           0, 240, 253, 251,   4,  20]], dtype=torch.int32)\n",
      "tensor([250,   4, 245, 250, 246,  19, 235, 244, 254,  10], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#################### Weight & Bias in HEX of Fully Connected Layer ####################\n",
    "\n",
    "print(np.shape(model.fc_1.weight))\n",
    "print((model.fc_1.weight * 128).int())\n",
    "\n",
    "print(np.shape(model.fc_1.bias))\n",
    "print((model.fc_1.bias * 128).int())\n",
    "\n",
    "int_fc_weight = (model.fc_1.weight * 128).int()\n",
    "int_fc_bias = (model.fc_1.bias * 128).int()\n",
    "\n",
    "# 2's Complement\n",
    "for i in range(10):\n",
    "    for j in range(48):\n",
    "        if int_fc_weight[i][j] < 0 :\n",
    "            int_fc_weight[i][j] += 256\n",
    "    if int_fc_bias[i] < 0 :\n",
    "        int_fc_bias[i] += 256\n",
    "        \n",
    "print(int_fc_weight)\n",
    "print(int_fc_bias)\n",
    "\n",
    "np.savetxt('fc_weight.mem', int_fc_weight, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('fc_bias.mem', int_fc_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab434a-e9e9-4ef1-b286-00fe201984fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
